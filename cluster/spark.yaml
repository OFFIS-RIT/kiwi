services:
  gpt-oss:
    image: scitrera/dgx-spark-vllm:0.14.0rc2-t5
    container_name: gpt-oss
    ports:
      - "45001:30000"
    volumes:
      - ./models:/root/.cache/huggingface
    environment:
      - HF_TOKEN=<HF_TOKEN>
      - VLLM_NO_USAGE_STATS=1
    shm_size: 32g
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      vllm serve openai/gpt-oss-20b
      --host 0.0.0.0
      --port 30000
      --gpu-memory-utilization 0.3
      --max-model-len 8192
      --max-num-batched-tokens 8192
      --max-num-seqs 16
      --enable-prefix-caching
      --tool-call-parser openai
      --async-scheduling
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:30000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 240s
    restart: unless-stopped

  embed:
    image: scitrera/dgx-spark-vllm:0.14.0rc2-t5
    container_name: embed
    ports:
      - "45002:30000"
    volumes:
      - ./models:/root/.cache/huggingface
    environment:
      - HF_TOKEN=<HF_TOKEN>
      - VLLM_NO_USAGE_STATS=1
    shm_size: 32g
    ipc: host
    depends_on:
      gpt-oss:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      vllm serve Qwen/Qwen3-Embedding-0.6B
      --host 0.0.0.0
      --port 30000
      --gpu-memory-utilization 0.1
      --max-model-len 2048
      --max-num-batched-tokens 16384
      --max-num-seqs 128
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:30000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  ocr:
    image: scitrera/dgx-spark-vllm:0.14.0rc2-t5
    container_name: ocr
    ports:
      - "45003:30000"
    volumes:
      - ./models:/root/.cache/huggingface
    environment:
      - HF_TOKEN=<HF_TOKEN>
      - VLLM_NO_USAGE_STATS=1
    shm_size: 32g
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      embed:
        condition: service_healthy
    command: >
      vllm serve Qwen/Qwen3-VL-4B-Instruct-FP8
      --host 0.0.0.0
      --port 30000
      --gpu-memory-utilization 0.25
      --max-model-len 4096
      --max-num-batched-tokens 8192
      --max-num-seqs 16
      --limit-mm-per-prompt '{"image":1,"video":0}'
      --enable-prefix-caching
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:30000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 240s
    restart: unless-stopped

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm
    ports:
      - "45000:4000"
    volumes:
      - ./litellm_spark.yaml:/app/config.yaml
    depends_on:
      gpt-oss:
        condition: service_healthy
      embed:
        condition: service_healthy
      ocr:
        condition: service_healthy
    command: --config /app/config.yaml
    restart: unless-stopped

networks:
  default:
    external: true
    driver: bridge
    name: kiwi_internal
